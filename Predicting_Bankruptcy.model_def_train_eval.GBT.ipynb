{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# Modeling using Gradient Boosted Trees\n\nThis notebook uses Python 3.7 with Spark 3.0\n\n## Begin by importing the bankruptcy data from Cloud Object Storage"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Import bankruptcy data from COS\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_1c720e1b65aa412b89762bf230a6b5f6 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='7GN1Hk1VrQ0_RGdupGi9ZaphffKXQb6iMJPgYn1DNqgh',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_1c720e1b65aa412b89762bf230a6b5f6.get_object(Bucket='ibmcapstoneproject-donotdelete-pr-iiaol8yd9vtgm6',Key='data.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data = pd.read_csv(body)\n\n\n\ndf_data.head()\n",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20210212185649-0000\nKERNEL_ID = be088286-4c32-402a-860c-dc6018c228a1\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 1,
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bankrupt?</th>\n      <th>ROA(C) before interest and depreciation before interest</th>\n      <th>ROA(A) before interest and % after tax</th>\n      <th>ROA(B) before interest and depreciation after tax</th>\n      <th>operating gross margin</th>\n      <th>realized sales gross margin</th>\n      <th>operating profit rate</th>\n      <th>tax Pre-net interest rate</th>\n      <th>after-tax net interest rate</th>\n      <th>non-industry income and expenditure/revenue</th>\n      <th>...</th>\n      <th>net income to total assets</th>\n      <th>total assets to GNP price</th>\n      <th>No-credit interval</th>\n      <th>Gross profit to Sales</th>\n      <th>Net income to stockholder's Equity</th>\n      <th>liability to equity</th>\n      <th>Degree of financial leverage (DFL)</th>\n      <th>Interest coverage ratio( Interest expense to EBIT )</th>\n      <th>one if net income was negative for the last two year zero otherwise</th>\n      <th>equity to liability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.370594</td>\n      <td>0.424389</td>\n      <td>0.405750</td>\n      <td>0.601457</td>\n      <td>0.601457</td>\n      <td>0.998969</td>\n      <td>0.796887</td>\n      <td>0.808809</td>\n      <td>0.302646</td>\n      <td>...</td>\n      <td>0.716845</td>\n      <td>0.009219</td>\n      <td>0.622879</td>\n      <td>0.601453</td>\n      <td>0.827890</td>\n      <td>0.290202</td>\n      <td>0.026601</td>\n      <td>0.564050</td>\n      <td>1</td>\n      <td>0.016469</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.464291</td>\n      <td>0.538214</td>\n      <td>0.516730</td>\n      <td>0.610235</td>\n      <td>0.610235</td>\n      <td>0.998946</td>\n      <td>0.797380</td>\n      <td>0.809301</td>\n      <td>0.303556</td>\n      <td>...</td>\n      <td>0.795297</td>\n      <td>0.008323</td>\n      <td>0.623652</td>\n      <td>0.610237</td>\n      <td>0.839969</td>\n      <td>0.283846</td>\n      <td>0.264577</td>\n      <td>0.570175</td>\n      <td>1</td>\n      <td>0.020794</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.426071</td>\n      <td>0.499019</td>\n      <td>0.472295</td>\n      <td>0.601450</td>\n      <td>0.601364</td>\n      <td>0.998857</td>\n      <td>0.796403</td>\n      <td>0.808388</td>\n      <td>0.302035</td>\n      <td>...</td>\n      <td>0.774670</td>\n      <td>0.040003</td>\n      <td>0.623841</td>\n      <td>0.601449</td>\n      <td>0.836774</td>\n      <td>0.290189</td>\n      <td>0.026555</td>\n      <td>0.563706</td>\n      <td>1</td>\n      <td>0.016474</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.399844</td>\n      <td>0.451265</td>\n      <td>0.457733</td>\n      <td>0.583541</td>\n      <td>0.583541</td>\n      <td>0.998700</td>\n      <td>0.796967</td>\n      <td>0.808966</td>\n      <td>0.303350</td>\n      <td>...</td>\n      <td>0.739555</td>\n      <td>0.003252</td>\n      <td>0.622929</td>\n      <td>0.583538</td>\n      <td>0.834697</td>\n      <td>0.281721</td>\n      <td>0.026697</td>\n      <td>0.564663</td>\n      <td>1</td>\n      <td>0.023982</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.465022</td>\n      <td>0.538432</td>\n      <td>0.522298</td>\n      <td>0.598783</td>\n      <td>0.598783</td>\n      <td>0.998973</td>\n      <td>0.797366</td>\n      <td>0.809304</td>\n      <td>0.303475</td>\n      <td>...</td>\n      <td>0.795016</td>\n      <td>0.003878</td>\n      <td>0.623521</td>\n      <td>0.598782</td>\n      <td>0.839973</td>\n      <td>0.278514</td>\n      <td>0.024752</td>\n      <td>0.575617</td>\n      <td>1</td>\n      <td>0.035490</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 96 columns</p>\n</div>",
                        "text/plain": "   Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n0          1                                           0.370594          \n1          1                                           0.464291          \n2          1                                           0.426071          \n3          1                                           0.399844          \n4          1                                           0.465022          \n\n    ROA(A) before interest and % after tax  \\\n0                                 0.424389   \n1                                 0.538214   \n2                                 0.499019   \n3                                 0.451265   \n4                                 0.538432   \n\n    ROA(B) before interest and depreciation after tax  \\\n0                                           0.405750    \n1                                           0.516730    \n2                                           0.472295    \n3                                           0.457733    \n4                                           0.522298    \n\n    operating gross margin   realized sales gross margin  \\\n0                 0.601457                      0.601457   \n1                 0.610235                      0.610235   \n2                 0.601450                      0.601364   \n3                 0.583541                      0.583541   \n4                 0.598783                      0.598783   \n\n    operating profit rate   tax Pre-net interest rate  \\\n0                0.998969                    0.796887   \n1                0.998946                    0.797380   \n2                0.998857                    0.796403   \n3                0.998700                    0.796967   \n4                0.998973                    0.797366   \n\n    after-tax net interest rate   non-industry income and expenditure/revenue  \\\n0                      0.808809                                      0.302646   \n1                      0.809301                                      0.303556   \n2                      0.808388                                      0.302035   \n3                      0.808966                                      0.303350   \n4                      0.809304                                      0.303475   \n\n   ...  net income to total assets  total assets to GNP price  \\\n0  ...                    0.716845                   0.009219   \n1  ...                    0.795297                   0.008323   \n2  ...                    0.774670                   0.040003   \n3  ...                    0.739555                   0.003252   \n4  ...                    0.795016                   0.003878   \n\n   No-credit interval  Gross profit to Sales  \\\n0            0.622879               0.601453   \n1            0.623652               0.610237   \n2            0.623841               0.601449   \n3            0.622929               0.583538   \n4            0.623521               0.598782   \n\n   Net income to stockholder's Equity  liability to equity  \\\n0                            0.827890             0.290202   \n1                            0.839969             0.283846   \n2                            0.836774             0.290189   \n3                            0.834697             0.281721   \n4                            0.839973             0.278514   \n\n   Degree of financial leverage (DFL)  \\\n0                            0.026601   \n1                            0.264577   \n2                            0.026555   \n3                            0.026697   \n4                            0.024752   \n\n   Interest coverage ratio( Interest expense to EBIT )  \\\n0                                           0.564050     \n1                                           0.570175     \n2                                           0.563706     \n3                                           0.564663     \n4                                           0.575617     \n\n   one if net income was negative for the last two year zero otherwise  \\\n0                                                  1                     \n1                                                  1                     \n2                                                  1                     \n3                                                  1                     \n4                                                  1                     \n\n   equity to liability  \n0             0.016469  \n1             0.020794  \n2             0.016474  \n3             0.023982  \n4             0.035490  \n\n[5 rows x 96 columns]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Next, import the features dataset and Choose a value for N"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nbody = client_1c720e1b65aa412b89762bf230a6b5f6.get_object(Bucket='ibmcapstoneproject-donotdelete-pr-iiaol8yd9vtgm6',Key='features_final.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nfeatures_final = pd.read_csv(body)\n\n# Due to the nature of IBM Watson Studio and difficulties loading data, this next step is necessary\ndf_data.to_csv(\"data.csv\")\nfeatures_final.to_csv(\"features_final.csv\")\n\n# Choose value for N\nN = 5\n\n# Select top N features\nfeatures_final = features_final.nlargest(N, 'avg_score', keep='first')\n\n# Define column names for model inputs\ncols = features_final['Feature Name'].values.tolist()\ncols",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "[' Persistent EPS in the Last Four Seasons',\n ' Per Share Net profit before tax (yuan)',\n ' net profit before tax/paid-in capital',\n 'net income to total assets',\n ' ROA(A) before interest and % after tax']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Now, onto prepping the data and building the model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Define SparkSession\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Load data into Spark DataFrame\ndf = spark.read.option(\"header\", True) \\\n               .option(\"inferSchema\", True).csv('data.csv')",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Define column names for the actual model\ncols = features_final['Feature Name'].values.tolist()\n\n# Create training and testing data\nsplits = df.randomSplit([0.75, 0.25])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Gradient Boosted Trees model\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol='Bankrupt?', outputCol='label')\nencoder = OneHotEncoder(inputCol='label', outputCol = 'labelVec')\nvectorAssembler = VectorAssembler(inputCols=cols, outputCol='features')\nnormalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\n\nfrom pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol='label', featuresCol='features', maxIter=10)\n\nfrom pyspark.ml import Pipeline\n\n##left the encoder out of the pipeline for some reason\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,gbt])\n\n#Evaluate training data\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_train)\n\n#binEval = BinaryClassificationEvaluator().setMetricName('areaUnderROC').setRawPredictionCol('prediction').setLabelCol('label')\nbinEval = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nprint(\"Train Data Accuracy: \", binEval.evaluate(prediction))\n\nmodel = pipeline.fit(df_test)\nprediction = model.transform(df_test)\n\nbinEval = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nprint(\"Test Data Accuracy: \", binEval.evaluate(prediction))",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train Data Accuracy:  0.9464955175224126\nTest Data Accuracy:  0.9640288156550724\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Hyperparameter tuning\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n# Create training and testing data\nsplits = df.randomSplit([0.75, 0.25])\ndf_train = splits[0]\ndf_test = splits[1]\n\n# Choose values for N\nN = [3, 5, 7, 10]\n\nfor i in range(len(N)):\n    \n    features_final = pd.read_csv('features_final.csv')\n\n    # Select top N features\n    features_final_1 = features_final.nlargest(N[i], 'avg_score', keep='first')\n    \n    # Define column names for model inputs\n    cols = features_final_1['Feature Name'].values.tolist()\n    \n    indexer = StringIndexer(inputCol='Bankrupt?', outputCol='label')\n    encoder = OneHotEncoder(inputCol='label', outputCol = 'labelVec')\n    vectorAssembler = VectorAssembler(inputCols=cols, outputCol='features')\n    normalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\n\n    from pyspark.ml.classification import GBTClassifier\n    gbt = GBTClassifier(labelCol='label', featuresCol='features', maxIter=10)\n\n    from pyspark.ml import Pipeline\n\n    ##left the encoder out of the pipeline for some reason\n    pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,gbt])\n\n    #Evaluate training data\n    from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\n    model = pipeline.fit(df_train)\n    prediction = model.transform(df_train)\n\n    #binEval = BinaryClassificationEvaluator().setMetricName('areaUnderROC').setRawPredictionCol('prediction').setLabelCol('label')\n    binEval = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n    print(\"Train Data Accuracy for N= \" + str(N[i]) + \":\", binEval.evaluate(prediction))\n\n    model = pipeline.fit(df_test)\n    prediction = model.transform(df_test)\n\n    binEval = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n    print(\"Test Data Accuracy for N= \" + str(N[i]) + \":\", binEval.evaluate(prediction))",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train Data Accuracy for N= 3: 0.9212279718537892\nTest Data Accuracy for N= 3: 0.9447044212617984\nTrain Data Accuracy for N= 5: 0.9448900800746228\nTest Data Accuracy for N= 5: 0.9712816691505217\nTrain Data Accuracy for N= 7: 0.9600440213447606\nTest Data Accuracy for N= 7: 0.9707650273224047\nTrain Data Accuracy for N= 10: 0.9698274602200299\nTest Data Accuracy for N= 10: 0.9938052657724791\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Summary\n\nThe hyperparameter tuning result are as follows:\n\nTrain Data Accuracy for N= 3: 0.9212279718537892\nTest Data Accuracy for N= 3: 0.9447044212617984\n\nTrain Data Accuracy for N= 5: 0.9448900800746228\nTest Data Accuracy for N= 5: 0.9712816691505217\n\nTrain Data Accuracy for N= 7: 0.9600440213447606\nTest Data Accuracy for N= 7: 0.9707650273224047\n\nTrain Data Accuracy for N= 10: 0.9698274602200299\nTest Data Accuracy for N= 10: 0.9938052657724791\n\nAs we can see, the ***optimal value is N=10.***\n\n\n### The only financial datapoints we need for the model are:\n\n1) Persistent EPS in the Last Four Seasons\n\n2) Per Share Net profit before tax (yuan)\n\n3) net profit before tax/paid-in capital\n\n4) net income to total assets\n\n5) ROA(A)before interest and % after tax\n\n6) ROA(B) before interest and depreciation after tax\n\n7) per Net Share Value (B)\n\n8) Net income to stockholder's Equity\n\n9) Net Value Per Share (A)\n\n10) net worth/assets\n\n## Success! \n\nUsing Gradient Boosted Trees, we have improved the model's accuracy to 99% on the test data. The Train data and Test data accuracies do not suggest overfitting or any other issues with the model. Using N=10, we can efficiently predict whether or not a company is at risk of going bankrupt. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python37",
            "display_name": "Python 3.7 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}