{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "# Initial exploration using a Support Vector Machine Model\n\nThis notebook uses Python 3.7 with Spark 3.0\n\n## Begin by importing the bankruptcy data from Cloud Object Storage"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# Import bankruptcy data from COS\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_1c720e1b65aa412b89762bf230a6b5f6 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='7GN1Hk1VrQ0_RGdupGi9ZaphffKXQb6iMJPgYn1DNqgh',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_1c720e1b65aa412b89762bf230a6b5f6.get_object(Bucket='ibmcapstoneproject-donotdelete-pr-iiaol8yd9vtgm6',Key='data.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data = pd.read_csv(body)\n\n\n\ndf_data.head()\n",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bankrupt?</th>\n      <th>ROA(C) before interest and depreciation before interest</th>\n      <th>ROA(A) before interest and % after tax</th>\n      <th>ROA(B) before interest and depreciation after tax</th>\n      <th>operating gross margin</th>\n      <th>realized sales gross margin</th>\n      <th>operating profit rate</th>\n      <th>tax Pre-net interest rate</th>\n      <th>after-tax net interest rate</th>\n      <th>non-industry income and expenditure/revenue</th>\n      <th>...</th>\n      <th>net income to total assets</th>\n      <th>total assets to GNP price</th>\n      <th>No-credit interval</th>\n      <th>Gross profit to Sales</th>\n      <th>Net income to stockholder's Equity</th>\n      <th>liability to equity</th>\n      <th>Degree of financial leverage (DFL)</th>\n      <th>Interest coverage ratio( Interest expense to EBIT )</th>\n      <th>one if net income was negative for the last two year zero otherwise</th>\n      <th>equity to liability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.370594</td>\n      <td>0.424389</td>\n      <td>0.405750</td>\n      <td>0.601457</td>\n      <td>0.601457</td>\n      <td>0.998969</td>\n      <td>0.796887</td>\n      <td>0.808809</td>\n      <td>0.302646</td>\n      <td>...</td>\n      <td>0.716845</td>\n      <td>0.009219</td>\n      <td>0.622879</td>\n      <td>0.601453</td>\n      <td>0.827890</td>\n      <td>0.290202</td>\n      <td>0.026601</td>\n      <td>0.564050</td>\n      <td>1</td>\n      <td>0.016469</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.464291</td>\n      <td>0.538214</td>\n      <td>0.516730</td>\n      <td>0.610235</td>\n      <td>0.610235</td>\n      <td>0.998946</td>\n      <td>0.797380</td>\n      <td>0.809301</td>\n      <td>0.303556</td>\n      <td>...</td>\n      <td>0.795297</td>\n      <td>0.008323</td>\n      <td>0.623652</td>\n      <td>0.610237</td>\n      <td>0.839969</td>\n      <td>0.283846</td>\n      <td>0.264577</td>\n      <td>0.570175</td>\n      <td>1</td>\n      <td>0.020794</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.426071</td>\n      <td>0.499019</td>\n      <td>0.472295</td>\n      <td>0.601450</td>\n      <td>0.601364</td>\n      <td>0.998857</td>\n      <td>0.796403</td>\n      <td>0.808388</td>\n      <td>0.302035</td>\n      <td>...</td>\n      <td>0.774670</td>\n      <td>0.040003</td>\n      <td>0.623841</td>\n      <td>0.601449</td>\n      <td>0.836774</td>\n      <td>0.290189</td>\n      <td>0.026555</td>\n      <td>0.563706</td>\n      <td>1</td>\n      <td>0.016474</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.399844</td>\n      <td>0.451265</td>\n      <td>0.457733</td>\n      <td>0.583541</td>\n      <td>0.583541</td>\n      <td>0.998700</td>\n      <td>0.796967</td>\n      <td>0.808966</td>\n      <td>0.303350</td>\n      <td>...</td>\n      <td>0.739555</td>\n      <td>0.003252</td>\n      <td>0.622929</td>\n      <td>0.583538</td>\n      <td>0.834697</td>\n      <td>0.281721</td>\n      <td>0.026697</td>\n      <td>0.564663</td>\n      <td>1</td>\n      <td>0.023982</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.465022</td>\n      <td>0.538432</td>\n      <td>0.522298</td>\n      <td>0.598783</td>\n      <td>0.598783</td>\n      <td>0.998973</td>\n      <td>0.797366</td>\n      <td>0.809304</td>\n      <td>0.303475</td>\n      <td>...</td>\n      <td>0.795016</td>\n      <td>0.003878</td>\n      <td>0.623521</td>\n      <td>0.598782</td>\n      <td>0.839973</td>\n      <td>0.278514</td>\n      <td>0.024752</td>\n      <td>0.575617</td>\n      <td>1</td>\n      <td>0.035490</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 96 columns</p>\n</div>",
                        "text/plain": "   Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n0          1                                           0.370594          \n1          1                                           0.464291          \n2          1                                           0.426071          \n3          1                                           0.399844          \n4          1                                           0.465022          \n\n    ROA(A) before interest and % after tax  \\\n0                                 0.424389   \n1                                 0.538214   \n2                                 0.499019   \n3                                 0.451265   \n4                                 0.538432   \n\n    ROA(B) before interest and depreciation after tax  \\\n0                                           0.405750    \n1                                           0.516730    \n2                                           0.472295    \n3                                           0.457733    \n4                                           0.522298    \n\n    operating gross margin   realized sales gross margin  \\\n0                 0.601457                      0.601457   \n1                 0.610235                      0.610235   \n2                 0.601450                      0.601364   \n3                 0.583541                      0.583541   \n4                 0.598783                      0.598783   \n\n    operating profit rate   tax Pre-net interest rate  \\\n0                0.998969                    0.796887   \n1                0.998946                    0.797380   \n2                0.998857                    0.796403   \n3                0.998700                    0.796967   \n4                0.998973                    0.797366   \n\n    after-tax net interest rate   non-industry income and expenditure/revenue  \\\n0                      0.808809                                      0.302646   \n1                      0.809301                                      0.303556   \n2                      0.808388                                      0.302035   \n3                      0.808966                                      0.303350   \n4                      0.809304                                      0.303475   \n\n   ...  net income to total assets  total assets to GNP price  \\\n0  ...                    0.716845                   0.009219   \n1  ...                    0.795297                   0.008323   \n2  ...                    0.774670                   0.040003   \n3  ...                    0.739555                   0.003252   \n4  ...                    0.795016                   0.003878   \n\n   No-credit interval  Gross profit to Sales  \\\n0            0.622879               0.601453   \n1            0.623652               0.610237   \n2            0.623841               0.601449   \n3            0.622929               0.583538   \n4            0.623521               0.598782   \n\n   Net income to stockholder's Equity  liability to equity  \\\n0                            0.827890             0.290202   \n1                            0.839969             0.283846   \n2                            0.836774             0.290189   \n3                            0.834697             0.281721   \n4                            0.839973             0.278514   \n\n   Degree of financial leverage (DFL)  \\\n0                            0.026601   \n1                            0.264577   \n2                            0.026555   \n3                            0.026697   \n4                            0.024752   \n\n   Interest coverage ratio( Interest expense to EBIT )  \\\n0                                           0.564050     \n1                                           0.570175     \n2                                           0.563706     \n3                                           0.564663     \n4                                           0.575617     \n\n   one if net income was negative for the last two year zero otherwise  \\\n0                                                  1                     \n1                                                  1                     \n2                                                  1                     \n3                                                  1                     \n4                                                  1                     \n\n   equity to liability  \n0             0.016469  \n1             0.020794  \n2             0.016474  \n3             0.023982  \n4             0.035490  \n\n[5 rows x 96 columns]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Next, import the features dataset and Choose a value for N"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "\nbody = client_1c720e1b65aa412b89762bf230a6b5f6.get_object(Bucket='ibmcapstoneproject-donotdelete-pr-iiaol8yd9vtgm6',Key='features_final.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nfeatures_final = pd.read_csv(body)\n\n# Due to the nature of IBM Watson Studio and difficulties loading data, this next step is necessary\ndf_data.to_csv(\"data.csv\")\nfeatures_final.to_csv(\"features_final.csv\")\n\n# Choose value for N\nN = 15\n\n# Select top N features\nfeatures_final = features_final.nlargest(N, 'avg_score', keep='first')\n\n# Define column names for model inputs\ncols = features_final['Feature Name'].values.tolist()\ncols",
            "execution_count": 32,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 32,
                    "data": {
                        "text/plain": "[' Persistent EPS in the Last Four Seasons',\n ' Per Share Net profit before tax (yuan)',\n ' net profit before tax/paid-in capital',\n 'net income to total assets',\n ' ROA(A) before interest and % after tax',\n ' ROA(B) before interest and depreciation after tax',\n ' per Net Share Value (B)',\n \"Net income to stockholder's Equity\",\n ' Net Value Per Share (A)',\n ' net worth/assets',\n 'Retained Earnings/Total assets',\n ' ROA(C) before interest and depreciation before interest',\n ' debt ratio %',\n ' Net Value Per Share (C)',\n ' borrowing dependency']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Define SparkSession\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Load data into Spark DataFrame\ndf = spark.read.option(\"header\", True) \\\n               .option(\"inferSchema\", True).csv('data.csv')",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Define column names for the actual model\ncols = features_final['Feature Name'].values.tolist()",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Create training and testing data\nsplits = df.randomSplit([0.75, 0.25])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Support Vector Machine model\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml import Pipeline\n\nindexer = StringIndexer(inputCol='Bankrupt?', outputCol='label')\nencoder = OneHotEncoder(inputCol='label', outputCol = 'labelVec')\nvectorAssembler = VectorAssembler(inputCols=cols, outputCol='features')\nnormalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\n\nlsvc = LinearSVC(maxIter=40, regParam=0.1)\n\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer, lsvc])\n\nmodel = pipeline.fit(df_train)\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate train data\nprediction = model.transform(df_train)\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)\n\nprint(\"Train Data Accuracy: \", evaluator.evaluate(prediction))\n\n# Evaluate test data\nprediction = model.transform(df_test)\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)\nprint(\"Test Data Accuracy: \", evaluator.evaluate(prediction))",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train Data Accuracy:  0.9200302393661126\nTest Data Accuracy:  0.9356821589205414\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Summary\n\nWe get 92% accuracy for our Training Data and 94% accuracy for our Testing data.\n\n### The good news:\n1) Low risk of overfitting, since the Training and Testing accuracies are similar.\n\n2) We are on the right track with our feature selection, since we are getting good accuracy with 15 features.\n\n### The bad news:\n1) This level of accuracy is still too low. With so many features and a simple binary classification, it should be possible to get >95% accuracy.\n\n### Improvement ideas:\n1) Tune hyperparameters, especially N"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Support Vector Machine model hyperparameter tuning over values for N\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.classification import LinearSVC\nfrom pyspark.ml import Pipeline\n\n# Create training and testing data\nsplits = df.randomSplit([0.75, 0.25])\ndf_train = splits[0]\ndf_test = splits[1]\n\n# Choose values for N\nN = [5, 10, 15, 30, 60, 90]\n\nfor i in range(len(N)):\n    \n    features_final = pd.read_csv('features_final.csv')\n\n    # Select top N features\n    features_final_1 = features_final.nlargest(N[i], 'avg_score', keep='first')\n    \n    # Define column names for model inputs\n    cols = features_final_1['Feature Name'].values.tolist()\n    #print(len(cols))\n\n    indexer = StringIndexer(inputCol='Bankrupt?', outputCol='label')\n    encoder = OneHotEncoder(inputCol='label', outputCol = 'labelVec')\n    vectorAssembler = VectorAssembler(inputCols=cols, outputCol='features')\n    normalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\n\n    lsvc = LinearSVC(maxIter=10, regParam=0.1)\n\n    pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer, lsvc])\n\n    model = pipeline.fit(df_train)\n\n    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n    # Evaluate train data\n    prediction = model.transform(df_train)\n\n    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n    #evaluator.evaluate(prediction)\n\n    print(\"Train Data Accuracy for N= \" + str(N[i]) + \":\", evaluator.evaluate(prediction))\n\n# Evaluate test data\n    prediction = model.transform(df_test)\n\n    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n    #evaluator.evaluate(prediction)\n    print(\"Test Data Accuracy for N= \" + str(N[i]) + \":\", evaluator.evaluate(prediction))",
            "execution_count": 38,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train Data Accuracy for N= 5: 0.8809944524347648\nTest Data Accuracy for N= 5: 0.8883885039774162\nTrain Data Accuracy for N= 10: 0.8923079632510468\nTest Data Accuracy for N= 10: 0.9129586861688461\nTrain Data Accuracy for N= 15: 0.9210014969620463\nTest Data Accuracy for N= 15: 0.9182448036951484\nTrain Data Accuracy for N= 30: 0.9028183979570881\nTest Data Accuracy for N= 30: 0.9142288940210387\nTrain Data Accuracy for N= 60: 0.9035357656520637\nTest Data Accuracy for N= 60: 0.8971003335899402\nTrain Data Accuracy for N= 90: 0.9175191523085495\nTest Data Accuracy for N= 90: 0.9126507569925568\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Summary of Hyperparameter Tuning\n\nThe results of Hyperparameter tuning are as follows:\n\nTrain Data Accuracy for N= 5: 0.8809944524347648\nTest Data Accuracy for N= 5: 0.8883885039774162\n\nTrain Data Accuracy for N= 10: 0.8923079632510468\nTest Data Accuracy for N= 10: 0.9129586861688461\n\nTrain Data Accuracy for N= 15: 0.9210014969620463\nTest Data Accuracy for N= 15: 0.9182448036951484\n\nTrain Data Accuracy for N= 30: 0.9028183979570881\nTest Data Accuracy for N= 30: 0.9142288940210387\n\nTrain Data Accuracy for N= 60: 0.9035357656520637\nTest Data Accuracy for N= 60: 0.8971003335899402\n\nTrain Data Accuracy for N= 90: 0.9175191523085495\nTest Data Accuracy for N= 90: 0.9126507569925568\n\n1) As we can see, improvements in accuracy level off around N=10\n\n2) While we do see slight improvements in N>10, one of our primary goals is to build a model that requires only readily available financial data to predict bankruptcy.\n\n3) At N=10, these features are:\n\n[' Persistent EPS in the Last Four Seasons',\n ' Per Share Net profit before tax (yuan)',\n ' net profit before tax/paid-in capital',\n 'net income to total assets',\n ' ROA(A) before interest and % after tax',\n ' ROA(B) before interest and depreciation after tax',\n ' per Net Share Value (B)',\n \"Net income to stockholder's Equity\",\n ' Net Value Per Share (A)',\n ' net worth/assets']\n \n4) At N=15, these features are: \n\n[' Persistent EPS in the Last Four Seasons',\n ' Per Share Net profit before tax (yuan)',\n ' net profit before tax/paid-in capital',\n 'net income to total assets',\n ' ROA(A) before interest and % after tax',\n ' ROA(B) before interest and depreciation after tax',\n ' per Net Share Value (B)',\n \"Net income to stockholder's Equity\",\n ' Net Value Per Share (A)',\n ' net worth/assets',\n 'Retained Earnings/Total assets',\n ' ROA(C) before interest and depreciation before interest',\n ' debt ratio %',\n ' Net Value Per Share (C)',\n ' borrowing dependency']\n \n In other words, as soon as we get to N>10, we start to get features such as \"Borrowing Dependency,\" which is not typically a readily available financial datapoint. \n \n As a result, our SVM model is optimized at N=10.\n \n As stated before, I think we can boost the N=10 accuracy value with other algorithms, so we'll try that next."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "features_final = pd.read_csv('features_final.csv')\n\nN=10\nholder = features_final.nlargest(N, 'avg_score', keep='first')\nholder['Feature Name'].values.tolist()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python37",
            "display_name": "Python 3.7 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}